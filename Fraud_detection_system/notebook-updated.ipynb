{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba78f4e-df21-4b99-bfa5-f6b9192d0d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.11/site-packages (2.228.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.36.3)\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (24.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.25.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.26.19)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.3 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.36.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (2024.12.14)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.22.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.3.9)\n",
      "Requirement already satisfied: pox>=0.3.5 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.70.17)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.11/site-packages (from referencing>=0.28.4->jsonschema->sagemaker) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn sagemaker boto3 xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feab4aa7-1dbf-4574-a399-893879d541bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.xgboost.estimator import XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75d555b-b645-41ab-a91b-3bc920d6efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual S3 bucket and prefix\n",
    "bucket = 'dataset-paysim'  # Remove 's3://' and trailing slash\n",
    "prefix = 'preprocessed-data/'  # Keep the prefix as it is\n",
    "\n",
    "s3_data_path = f's3://dataset-paysim/preprocessed-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d984585-f07a-4449-84ad-cf9fc039ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found in S3: ['preprocessed-data//preprocessed_data.csv']\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3') \n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    objects = response['Contents']\n",
    "    print(\"Objects found in S3:\", [obj['Key'] for obj in objects])  # Debugging step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187de769-986e-4a18-bb21-200ff562b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found in S3: ['preprocessed-data//preprocessed_data.csv']\n",
      "No valid Parquet files found in the given S3 prefix.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Corrected bucket name\n",
    "bucket = 'dataset-paysim'  # No 's3://' and no trailing '/'\n",
    "prefix = 'preprocessed-data/'  # Check if this is correct\n",
    "\n",
    "# List all files in the S3 prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    objects = response['Contents']\n",
    "    print(\"Objects found in S3:\", [obj['Key'] for obj in objects])  # Debugging step\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for obj in objects:\n",
    "        if obj['Key'].endswith('.parquet'):  # Adapt for your file type (e.g., '.csv')\n",
    "            file_path = obj['Key']\n",
    "            s3_file = s3.get_object(Bucket=bucket, Key=file_path)\n",
    "            df = pd.read_parquet(s3_file['Body'])  # Use pd.read_csv if it's a CSV\n",
    "            all_data.append(df)\n",
    "\n",
    "    if all_data:  # Only concatenate if data exists\n",
    "        df = pd.concat(all_data, ignore_index=True)\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"No valid Parquet files found in the given S3 prefix.\")\n",
    "else:\n",
    "    print(\"No files found in the given S3 prefix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5239beaa-b76e-4ea6-8101-6a7e44c9f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects found in S3: ['preprocessed-data//preprocessed_data.csv']\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64       170136.0       160296.36             0.0   \n",
      "1     1   1864.28        21249.0        19384.72             0.0   \n",
      "2     1    181.00          181.0            0.00             0.0   \n",
      "3     1    181.00          181.0            0.00         21182.0   \n",
      "4     1  11668.14        41554.0        29885.86             0.0   \n",
      "\n",
      "   newbalanceDest  isFraud  isFlaggedFraud  type_CASH_IN  type_CASH_OUT  \\\n",
      "0             0.0        0               0         False          False   \n",
      "1             0.0        0               0         False          False   \n",
      "2             0.0        1               0         False          False   \n",
      "3             0.0        1               0         False           True   \n",
      "4             0.0        0               0         False          False   \n",
      "\n",
      "   type_DEBIT  type_PAYMENT  type_TRANSFER  \n",
      "0       False          True          False  \n",
      "1       False          True          False  \n",
      "2       False         False           True  \n",
      "3       False         False          False  \n",
      "4       False          True          False  \n"
     ]
    }
   ],
   "source": [
    "# Corrected bucket name\n",
    "bucket = 'dataset-paysim'  # No 's3://' and no trailing '/'\n",
    "prefix = 'preprocessed-data/'  # Check if this is correct\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List all files in the S3 prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    objects = response['Contents']\n",
    "    print(\"Objects found in S3:\", [obj['Key'] for obj in objects])  # Debugging step\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    if obj['Key'].endswith('.csv'):\n",
    "        file_path = obj['Key']\n",
    "        s3_file = s3.get_object(Bucket=bucket, Key=file_path)\n",
    "        df = pd.read_csv(s3_file['Body'])  # Read CSV instead of Parquet\n",
    "        all_data.append(df)\n",
    "\n",
    "    if all_data:  # Only concatenate if data exists\n",
    "        df = pd.concat(all_data, ignore_index=True)\n",
    "        print(df.head())\n",
    "\n",
    "    else:\n",
    "        print(\"No valid Parquet files found in the given S3 prefix.\")\n",
    "else:\n",
    "    print(\"No files found in the given S3 prefix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf7780f-4578-40a2-a62d-a558624b9fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 5090096\n",
      "Test set size: 1272524\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "\n",
    "# df = pd.read_csv('path/to/your/data.csv')\n",
    "\n",
    "X = df.drop('isFraud', axis=1)  # Replace 'isFraud' with your target column name\n",
    "y = df['isFraud']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec9d179-de81-4631-aa0f-d17486f42081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to: s3://dataset-paysim/train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store the training data\n",
    "train_data_dir = 'train_data'\n",
    "os.makedirs(train_data_dir, exist_ok=True)  # Create if it doesn't exist\n",
    "\n",
    "# Combine X_train and y_train into a single DataFrame for saving\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Save the training data to a CSV file\n",
    "train_df.to_csv(os.path.join(train_data_dir, 'train.csv'), index=False)\n",
    "\n",
    "# Upload the training data to S3\n",
    "sagemaker_session = sagemaker.Session()\n",
    "train_data_s3_path = sagemaker_session.upload_data(path=train_data_dir, bucket=bucket, key_prefix='train')\n",
    "\n",
    "print(f\"Training data uploaded to: {train_data_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df51ef98-8ece-46c4-bd4c-36d31a8e29b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data uploaded to: s3://dataset-paysim/test\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store the test data\n",
    "test_data_dir = 'test_data'\n",
    "os.makedirs(test_data_dir, exist_ok=True)  # Create if it doesn't exist\n",
    "\n",
    "# Combine X_test and y_test into a single DataFrame for saving\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save the test data to a CSV file\n",
    "test_df.to_csv(os.path.join(test_data_dir, 'test.csv'), index=False)\n",
    "\n",
    "# Upload the training data to S3\n",
    "sagemaker_session = sagemaker.Session()\n",
    "test_data_s3_path = sagemaker_session.upload_data(path=test_data_dir, bucket=bucket, key_prefix='test')\n",
    "\n",
    "print(f\"Testing data uploaded to: {test_data_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb3fe35e-ade8-4c80-a40c-7dc5205fcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Define the training input channel\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_data_s3_path,\n",
    "    content_type='csv'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a038adae-9c2b-4e18-9ad7-8728d909c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sagemaker execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define the S3 output path for saving the model\n",
    "output_path = f's3://dataset-paysim/xgboost-model-output'\n",
    "\n",
    "# Configure the XGBoost estimator\n",
    "xgb = XGBoost(\n",
    "    entry_point='xgboost_script.py',  # training script (see below)\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    framework_version='1.0-1',  # Or the latest version\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    role=role,\n",
    "    objective='binary:logistic',\n",
    "    num_round=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c566b6e-80f0-46da-93a9-6bf38aef3c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-02-16-14-43-15-284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 14:43:17 Starting - Starting the training job...\n",
      "2025-02-16 14:43:46 Downloading - Downloading input data......\n",
      "2025-02-16 14:44:21 Downloading - Downloading the training image..\u001b[34m[2025-02-16 14:44:58.427 ip-10-0-152-8.ap-southeast-2.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Invoking user training script.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Module xgboost_script does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating setup.cfg\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: xgboost-script\n",
      "  Building wheel for xgboost-script (setup.py): started\n",
      "  Building wheel for xgboost-script (setup.py): finished with status 'done'\n",
      "  Created wheel for xgboost-script: filename=xgboost_script-1.0.0-py2.py3-none-any.whl size=6164 sha256=cf15b4c272ada9195a9e3804e75e6d1fef7b680f1e19e9e77b81051cdae6ce51\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-znfk3juq/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built xgboost-script\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xgboost-script\u001b[0m\n",
      "\u001b[34mSuccessfully installed xgboost-script-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2025-02-16-14-43-15-284\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://dataset-paysim/sagemaker-xgboost-2025-02-16-14-43-15-284/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"xgboost_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"xgboost_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=xgboost_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=xgboost_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://dataset-paysim/sagemaker-xgboost-2025-02-16-14-43-15-284/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2025-02-16-14-43-15-284\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://dataset-paysim/sagemaker-xgboost-2025-02-16-14-43-15-284/source/sourcedir.tar.gz\",\"module_name\":\"xgboost_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"xgboost_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python3.6/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m xgboost_script\u001b[0m\n",
      "\n",
      "2025-02-16 14:44:51 Training - Training image download completed. Training in progress.\u001b[34m[0]#011train-error:0.00065#011train-logloss:0.50799#011eval-error:0.00062#011eval-logloss:0.51435\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\u001b[0m\n",
      "\u001b[34mWill train until eval-logloss hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.00069#011train-logloss:0.39674#011eval-error:0.00069#011eval-logloss:0.39193\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.00091#011train-logloss:0.31014#011eval-error:0.00089#011eval-logloss:0.30593\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.00085#011train-logloss:0.23988#011eval-error:0.00083#011eval-logloss:0.24249\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.00067#011train-logloss:0.19244#011eval-error:0.00067#011eval-logloss:0.19258\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.00048#011train-logloss:0.15599#011eval-error:0.00047#011eval-logloss:0.15467\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.00042#011train-logloss:0.12559#011eval-error:0.00040#011eval-logloss:0.12443\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.00042#011train-logloss:0.10206#011eval-error:0.00040#011eval-logloss:0.10083\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.00041#011train-logloss:0.08188#011eval-error:0.00039#011eval-logloss:0.08197\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.00041#011train-logloss:0.06731#011eval-error:0.00039#011eval-logloss:0.06683\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00041#011train-logloss:0.05520#011eval-error:0.00039#011eval-logloss:0.05489\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.00041#011train-logloss:0.04463#011eval-error:0.00039#011eval-logloss:0.04486\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.00039#011train-logloss:0.03675#011eval-error:0.00037#011eval-logloss:0.03683\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00040#011train-logloss:0.03069#011eval-error:0.00038#011eval-logloss:0.03046\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.00039#011train-logloss:0.02510#011eval-error:0.00038#011eval-logloss:0.02514\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.00038#011train-logloss:0.02101#011eval-error:0.00037#011eval-logloss:0.02085\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.00038#011train-logloss:0.01731#011eval-error:0.00036#011eval-logloss:0.01735\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.00042#011train-logloss:0.01445#011eval-error:0.00039#011eval-logloss:0.01453\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.00042#011train-logloss:0.01231#011eval-error:0.00039#011eval-logloss:0.01220\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.00041#011train-logloss:0.01039#011eval-error:0.00039#011eval-logloss:0.01030\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.00042#011train-logloss:0.00876#011eval-error:0.00039#011eval-logloss:0.00874\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.00040#011train-logloss:0.00747#011eval-error:0.00037#011eval-logloss:0.00743\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.00039#011train-logloss:0.00644#011eval-error:0.00037#011eval-logloss:0.00638\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.00039#011train-logloss:0.00553#011eval-error:0.00037#011eval-logloss:0.00551\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.00035#011train-logloss:0.00478#011eval-error:0.00034#011eval-logloss:0.00477\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.00035#011train-logloss:0.00419#011eval-error:0.00034#011eval-logloss:0.00417\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.00035#011train-logloss:0.00367#011eval-error:0.00034#011eval-logloss:0.00367\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.00036#011train-logloss:0.00322#011eval-error:0.00035#011eval-logloss:0.00326\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.00035#011train-logloss:0.00293#011eval-error:0.00034#011eval-logloss:0.00294\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.00034#011train-logloss:0.00263#011eval-error:0.00034#011eval-logloss:0.00265\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.00034#011train-logloss:0.00238#011eval-error:0.00033#011eval-logloss:0.00241\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.00034#011train-logloss:0.00216#011eval-error:0.00032#011eval-logloss:0.00220\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.00034#011train-logloss:0.00198#011eval-error:0.00033#011eval-logloss:0.00202\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.00034#011train-logloss:0.00181#011eval-error:0.00033#011eval-logloss:0.00185\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.00033#011train-logloss:0.00169#011eval-error:0.00032#011eval-logloss:0.00172\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.00032#011train-logloss:0.00160#011eval-error:0.00032#011eval-logloss:0.00162\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.00032#011train-logloss:0.00148#011eval-error:0.00032#011eval-logloss:0.00152\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.00032#011train-logloss:0.00142#011eval-error:0.00032#011eval-logloss:0.00146\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.00031#011train-logloss:0.00134#011eval-error:0.00032#011eval-logloss:0.00138\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.00031#011train-logloss:0.00129#011eval-error:0.00031#011eval-logloss:0.00133\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.00031#011train-logloss:0.00125#011eval-error:0.00031#011eval-logloss:0.00129\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.00031#011train-logloss:0.00122#011eval-error:0.00031#011eval-logloss:0.00126\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.00031#011train-logloss:0.00118#011eval-error:0.00031#011eval-logloss:0.00123\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.00031#011train-logloss:0.00114#011eval-error:0.00031#011eval-logloss:0.00119\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.00031#011train-logloss:0.00112#011eval-error:0.00031#011eval-logloss:0.00117\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.00031#011train-logloss:0.00111#011eval-error:0.00031#011eval-logloss:0.00116\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.00030#011train-logloss:0.00107#011eval-error:0.00030#011eval-logloss:0.00113\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.00030#011train-logloss:0.00106#011eval-error:0.00030#011eval-logloss:0.00111\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.00030#011train-logloss:0.00103#011eval-error:0.00030#011eval-logloss:0.00110\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.00029#011train-logloss:0.00102#011eval-error:0.00030#011eval-logloss:0.00109\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.00029#011train-logloss:0.00102#011eval-error:0.00030#011eval-logloss:0.00108\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.00029#011train-logloss:0.00100#011eval-error:0.00030#011eval-logloss:0.00107\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.00029#011train-logloss:0.00100#011eval-error:0.00030#011eval-logloss:0.00106\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.00029#011train-logloss:0.00099#011eval-error:0.00030#011eval-logloss:0.00106\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.00028#011train-logloss:0.00096#011eval-error:0.00030#011eval-logloss:0.00103\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.00028#011train-logloss:0.00095#011eval-error:0.00029#011eval-logloss:0.00103\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.00027#011train-logloss:0.00093#011eval-error:0.00029#011eval-logloss:0.00100\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.00027#011train-logloss:0.00092#011eval-error:0.00029#011eval-logloss:0.00100\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.00027#011train-logloss:0.00091#011eval-error:0.00029#011eval-logloss:0.00099\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.00027#011train-logloss:0.00091#011eval-error:0.00029#011eval-logloss:0.00099\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.00026#011train-logloss:0.00090#011eval-error:0.00029#011eval-logloss:0.00098\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.00026#011train-logloss:0.00087#011eval-error:0.00029#011eval-logloss:0.00096\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.00025#011train-logloss:0.00085#011eval-error:0.00028#011eval-logloss:0.00094\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.00025#011train-logloss:0.00083#011eval-error:0.00028#011eval-logloss:0.00093\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.00025#011train-logloss:0.00082#011eval-error:0.00027#011eval-logloss:0.00092\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.00024#011train-logloss:0.00082#011eval-error:0.00027#011eval-logloss:0.00092\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.00024#011train-logloss:0.00080#011eval-error:0.00027#011eval-logloss:0.00090\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.00024#011train-logloss:0.00079#011eval-error:0.00027#011eval-logloss:0.00089\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.00024#011train-logloss:0.00079#011eval-error:0.00027#011eval-logloss:0.00089\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.00023#011train-logloss:0.00077#011eval-error:0.00026#011eval-logloss:0.00087\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.00023#011train-logloss:0.00076#011eval-error:0.00026#011eval-logloss:0.00086\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.00023#011train-logloss:0.00075#011eval-error:0.00026#011eval-logloss:0.00086\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.00023#011train-logloss:0.00074#011eval-error:0.00026#011eval-logloss:0.00085\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.00023#011train-logloss:0.00074#011eval-error:0.00026#011eval-logloss:0.00084\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.00023#011train-logloss:0.00072#011eval-error:0.00026#011eval-logloss:0.00083\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.00022#011train-logloss:0.00071#011eval-error:0.00025#011eval-logloss:0.00082\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.00022#011train-logloss:0.00070#011eval-error:0.00025#011eval-logloss:0.00081\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.00022#011train-logloss:0.00069#011eval-error:0.00025#011eval-logloss:0.00081\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.00022#011train-logloss:0.00069#011eval-error:0.00025#011eval-logloss:0.00080\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.00022#011train-logloss:0.00069#011eval-error:0.00025#011eval-logloss:0.00080\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.00022#011train-logloss:0.00068#011eval-error:0.00025#011eval-logloss:0.00080\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.00021#011train-logloss:0.00068#011eval-error:0.00025#011eval-logloss:0.00080\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.00021#011train-logloss:0.00068#011eval-error:0.00025#011eval-logloss:0.00080\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.00021#011train-logloss:0.00067#011eval-error:0.00024#011eval-logloss:0.00079\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.00021#011train-logloss:0.00066#011eval-error:0.00024#011eval-logloss:0.00078\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.00021#011train-logloss:0.00065#011eval-error:0.00024#011eval-logloss:0.00077\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.00020#011train-logloss:0.00064#011eval-error:0.00024#011eval-logloss:0.00077\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.00020#011train-logloss:0.00064#011eval-error:0.00024#011eval-logloss:0.00076\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.00020#011train-logloss:0.00063#011eval-error:0.00024#011eval-logloss:0.00076\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.00020#011train-logloss:0.00063#011eval-error:0.00023#011eval-logloss:0.00075\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.00020#011train-logloss:0.00062#011eval-error:0.00023#011eval-logloss:0.00075\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.00020#011train-logloss:0.00061#011eval-error:0.00023#011eval-logloss:0.00074\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.00019#011train-logloss:0.00061#011eval-error:0.00023#011eval-logloss:0.00074\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.00019#011train-logloss:0.00061#011eval-error:0.00023#011eval-logloss:0.00074\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.00019#011train-logloss:0.00060#011eval-error:0.00023#011eval-logloss:0.00073\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.00019#011train-logloss:0.00060#011eval-error:0.00023#011eval-logloss:0.00073\u001b[0m\n",
      "\n",
      "2025-02-16 14:48:53 Uploading - Uploading generated training model\u001b[34m[96]#011train-error:0.00019#011train-logloss:0.00059#011eval-error:0.00023#011eval-logloss:0.00073\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.00019#011train-logloss:0.00059#011eval-error:0.00023#011eval-logloss:0.00073\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.00019#011train-logloss:0.00058#011eval-error:0.00023#011eval-logloss:0.00072\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.00019#011train-logloss:0.00058#011eval-error:0.00023#011eval-logloss:0.00072\u001b[0m\n",
      "\u001b[34mAccuracy: 0.9998\u001b[0m\n",
      "\u001b[34mPrecision: 0.9763\u001b[0m\n",
      "\u001b[34mRecall: 0.8362\u001b[0m\n",
      "\u001b[34mF1 Score: 0.9008\u001b[0m\n",
      "\n",
      "2025-02-16 14:49:06 Completed - Training job completed\n",
      "Training seconds: 320\n",
      "Billable seconds: 320\n"
     ]
    }
   ],
   "source": [
    "xgb.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495b5b26-5f4d-48a5-bb54-19699bd947a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-02-16-14-50-53-188\n",
      "INFO:sagemaker:Creating endpoint-config with name fraud-detection-ep1\n",
      "INFO:sagemaker:Creating endpoint with name fraud-detection-ep1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!Model successfully deployed!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get the trained model location from S3 (Check this path in the SageMaker console)\n",
    "model_data_path = 's3://dataset-paysim/xgboost-model-output/sagemaker-xgboost-2025-02-16-06-29-54-221/output/model.tar.gz'  # Replace with actual path\n",
    "\n",
    "# Get the IAM role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create an XGBoost Model object\n",
    "xgb_model = XGBoostModel(\n",
    "    model_data=model_data_path,\n",
    "    role=role,\n",
    "    framework_version='1.0-1'\n",
    ")\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name='fraud-detection-ep1',\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(\"Model successfully deployed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a512086f-6a61-4f4b-9d52-5f5e1be4c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: InService\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "endpoint_name = \"fraud-detection-ep1\"  # Ensure this matches your deployment\n",
    "\n",
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(\"Endpoint Status:\", response[\"EndpointStatus\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d297a26e-1c66-4b54-b9dd-7fa2733465c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Prediction: 9.382477401231881e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=\"fraud-detection-ep1\",\n",
    "    serializer=CSVSerializer(),  # Ensures data is sent in correct format\n",
    "    deserializer=JSONDeserializer()  # Parses JSON response\n",
    ")\n",
    "\n",
    "# Ensure all data is numeric\n",
    "sample_data = X_test.iloc[:1].copy() # Select one row for prediction\n",
    "sample_data = sample_data.astype(float)  # Convert to float if necessary\n",
    "\n",
    "# Convert to numpy array\n",
    "sample_data_array = sample_data.to_numpy()\n",
    "\n",
    "# Convert to CSV format (without headers)\n",
    "payload = \",\".join(map(str, sample_data_array.flatten()))\n",
    "\n",
    "# Make prediction\n",
    "prediction = predictor.predict(payload)\n",
    "print(\"Fraud Prediction:\", prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd02fb10-4036-4176-a246-754c8afab87b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message \"Loading csv data failed with Exception, please ensure data is in csv format:\n <class 'ValueError'>\n could not convert string to float: 'True'\". See https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#logEventViewer:group=/aws/sagemaker/Endpoints/fraud-detection-ep in account 985539759309 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m sample_data_csv \u001b[38;5;241m=\u001b[39m sample_data\u001b[38;5;241m.\u001b[39mto_csv(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/base_predictor.py:212\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes, component_name)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference_component_name:\n\u001b[1;32m    210\u001b[0m     request_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferenceComponentName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inference_component_name\n\u001b[0;32m--> 212\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message \"Loading csv data failed with Exception, please ensure data is in csv format:\n <class 'ValueError'>\n could not convert string to float: 'True'\". See https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#logEventViewer:group=/aws/sagemaker/Endpoints/fraud-detection-ep in account 985539759309 for more information."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb5b9e04-a8fe-4185-b103-22ede8746cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Amazon CloudWatch for Real-Time Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56d58eb0-ce8e-4fee-bce7-7d22725185d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged prediction to CloudWatch.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize CloudWatch Logs\n",
    "logs_client = boto3.client(\"logs\")\n",
    "\n",
    "log_group = \"FraudDetectionLogs\"\n",
    "log_stream = \"Predictions\"\n",
    "\n",
    "# Create Log Group and Log Stream if not exists\n",
    "try:\n",
    "    logs_client.create_log_group(logGroupName=log_group)\n",
    "except logs_client.exceptions.ResourceAlreadyExistsException:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    logs_client.create_log_stream(logGroupName=log_group, logStreamName=log_stream)\n",
    "except logs_client.exceptions.ResourceAlreadyExistsException:\n",
    "    pass\n",
    "\n",
    "# Log fraud prediction\n",
    "log_event = {\n",
    "    \"timestamp\": int(datetime.utcnow().timestamp() * 1000),\n",
    "    \"message\": json.dumps({\"prediction\": prediction, \"timestamp\": str(datetime.utcnow())})\n",
    "}\n",
    "\n",
    "logs_client.put_log_events(\n",
    "    logGroupName=log_group,\n",
    "    logStreamName=log_stream,\n",
    "    logEvents=[log_event]\n",
    ")\n",
    "\n",
    "print(\"Logged prediction to CloudWatch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37d96a59-a5e7-49d4-81c8-d0f0671070f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions stored in S3 for Athena analysis.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import csv\n",
    "\n",
    "s3_bucket = \"dataset-paysim\"\n",
    "s3_path = \"fraud-detection-results/predictions.csv\"\n",
    "\n",
    "# Convert prediction to CSV format\n",
    "with open(\"/tmp/predictions.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"TransactionID\", \"Prediction\"])\n",
    "    writer.writerow([123456, prediction])  # Example transaction ID\n",
    "\n",
    "# Upload results to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(\"/tmp/predictions.csv\", s3_bucket, s3_path)\n",
    "print(\"Predictions stored in S3 for Athena analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff000e38-ea95-472d-9d87-b66dd231ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: fraud-detection-ep1\n",
      "INFO:sagemaker:Deleting endpoint with name: fraud-detection-ep1\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768b186-a9ff-415b-8669-460bece43803",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc62daa1-34c7-4f7d-807c-2f3ba377ea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: sagemaker-xgboost-250216-1235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................*\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for HyperParameterTuning job sagemaker-xgboost-250216-1235: Failed. Reason: No objective metrics found after running 3 training jobs. Please ensure that the custom algorithm is emitting the objective metric as defined by the regular expression provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m tuner \u001b[38;5;241m=\u001b[39m HyperparameterTuner(\n\u001b[1;32m     15\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb,\n\u001b[1;32m     16\u001b[0m     objective_metric_name\u001b[38;5;241m=\u001b[39mobjective_metric_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     max_parallel_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Fit the tuner with data channels\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_channels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/tuner.py:1044\u001b[0m, in \u001b[0;36mHyperparameterTuner.fit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_with_estimator_dict(inputs, job_name, include_cls_metadata, estimator_kwargs)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_tuning_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/tuner.py:2353\u001b[0m, in \u001b[0;36m_TuningJob.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Placeholder docstring.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_tuning_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5273\u001b[0m, in \u001b[0;36mSession.wait_for_tuning_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   5259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for an Amazon SageMaker hyperparameter tuning job to complete.\u001b[39;00m\n\u001b[1;32m   5260\u001b[0m \n\u001b[1;32m   5261\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5270\u001b[0m \u001b[38;5;124;03m    exceptions.UnexpectedStatusException: If the hyperparameter tuning job fails.\u001b[39;00m\n\u001b[1;32m   5271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5272\u001b[0m desc \u001b[38;5;241m=\u001b[39m _wait_until(\u001b[38;5;28;01mlambda\u001b[39;00m: _tuning_job_status(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client, job), poll)\n\u001b[0;32m-> 5273\u001b[0m \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHyperParameterTuningJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8508\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8504\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8505\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8506\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8507\u001b[0m     )\n\u001b[0;32m-> 8508\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8509\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8510\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8511\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8512\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for HyperParameterTuning job sagemaker-xgboost-250216-1235: Failed. Reason: No objective metrics found after running 3 training jobs. Please ensure that the custom algorithm is emitting the objective metric as defined by the regular expression provided."
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'num_round': IntegerParameter(50, 200),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'max_depth': IntegerParameter(3, 7)\n",
    "}\n",
    "\n",
    "# Objective metric name\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "# Create Hyperparameter Tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=xgb,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=1\n",
    ")\n",
    "\n",
    "# Fit the tuner with data channels\n",
    "tuner.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13a8bd-dea3-43bb-a751-61f96a980f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "258a614e-a64c-4c02-b137-d125776ff1b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Create a SageMaker Model object\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This represents your trained model and its inference environment.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mXGBoostModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 5. Create a unique endpoint name\u001b[39;00m\n\u001b[1;32m     12\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/xgboost/model.py:140\u001b[0m, in \u001b[0;36mXGBoostModel.__init__\u001b[0;34m(self, model_data, role, entry_point, framework_version, image_uri, py_version, predictor_cls, model_server_workers, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_server_workers \u001b[38;5;241m=\u001b[39m model_server_workers\n\u001b[1;32m    139\u001b[0m validate_py_version(py_version)\n\u001b[0;32m--> 140\u001b[0m \u001b[43mvalidate_framework_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframework_version\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/xgboost/utils.py:28\u001b[0m, in \u001b[0;36mvalidate_framework_version\u001b[0;34m(framework_version)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_framework_version\u001b[39m(framework_version):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Placeholder docstring\"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     xgboost_version \u001b[38;5;241m=\u001b[39m \u001b[43mframework_version\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xgboost_version \u001b[38;5;129;01min\u001b[39;00m defaults\u001b[38;5;241m.\u001b[39mXGBOOST_UNSUPPORTED_VERSIONS:\n\u001b[1;32m     30\u001b[0m         msg \u001b[38;5;241m=\u001b[39m defaults\u001b[38;5;241m.\u001b[39mXGBOOST_UNSUPPORTED_VERSIONS[xgboost_version]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# 4. Create a SageMaker Model object\n",
    "# This represents your trained model and its inference environment.\n",
    "\n",
    "model = XGBoostModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "# 5. Create a unique endpoint name\n",
    "timestamp = int(time.time())\n",
    "unique_endpoint_name = f\"sagemaker-xgboost-{timestamp}\"\n",
    "\n",
    "# 6. Deploy the model\n",
    "# This creates the endpoint configuration and the endpoint itself.\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name=unique_endpoint_name\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
